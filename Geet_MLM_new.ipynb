{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Geet_MLM_new.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"79f0508db1f8452aa9a6a5d23ba479ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42df603915ba4d4092a0f55122ddc820","IPY_MODEL_37c5898ba5304b808a04f3970d5a3f7b","IPY_MODEL_e5b5202f237842bb8f85d5ab2d371e1c"],"layout":"IPY_MODEL_3e3b5a12d1604c688dbdaffc5d69c35a"}},"42df603915ba4d4092a0f55122ddc820":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_686616f3f2594672bc9205a92395d946","placeholder":"​","style":"IPY_MODEL_dd5fdf42f5364e7190195d0b7b9b7a9e","value":"Downloading config.json: 100%"}},"37c5898ba5304b808a04f3970d5a3f7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_884d9d0c7b9642b5bbf9de6ff553c71f","max":1716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8127cf98d2684d6896dafc2eb8165ae5","value":1716}},"e5b5202f237842bb8f85d5ab2d371e1c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ea6bc532fd24061b7dead3c3b20ebf3","placeholder":"​","style":"IPY_MODEL_ba70f190eb524f0e959fe6209ccfed16","value":" 1.68k/1.68k [00:00&lt;00:00, 50.0kB/s]"}},"3e3b5a12d1604c688dbdaffc5d69c35a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"686616f3f2594672bc9205a92395d946":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd5fdf42f5364e7190195d0b7b9b7a9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"884d9d0c7b9642b5bbf9de6ff553c71f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8127cf98d2684d6896dafc2eb8165ae5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ea6bc532fd24061b7dead3c3b20ebf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba70f190eb524f0e959fe6209ccfed16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4149f29b603f45e39c3594a13d27f998":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da598b51787448fdae2982ae8efa287a","IPY_MODEL_1933f8962efe495ab22bc52bf070b4d0","IPY_MODEL_75b1251c04e54907832cd7d015747e83"],"layout":"IPY_MODEL_af1473e3dc3c42a09946b51a95cd45cd"}},"da598b51787448fdae2982ae8efa287a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_106e6d4fed904026b66de0645ad18e11","placeholder":"​","style":"IPY_MODEL_670e9a01c632436283649f44aa8423ad","value":"Downloading pytorch_model.bin: 100%"}},"1933f8962efe495ab22bc52bf070b4d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63305387d33340998a9e71171d4e5644","max":557771387,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abfb82a045e54cb4b6eef29e4cefd0c4","value":557771387}},"75b1251c04e54907832cd7d015747e83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba55ff41c43643e08f6b9e19709c2cd6","placeholder":"​","style":"IPY_MODEL_0c7a1792669f41cd860c82e8f4035fad","value":" 532M/532M [00:09&lt;00:00, 63.5MB/s]"}},"af1473e3dc3c42a09946b51a95cd45cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"106e6d4fed904026b66de0645ad18e11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"670e9a01c632436283649f44aa8423ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63305387d33340998a9e71171d4e5644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abfb82a045e54cb4b6eef29e4cefd0c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba55ff41c43643e08f6b9e19709c2cd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c7a1792669f41cd860c82e8f4035fad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8416b6d82fe44ae0b3af2b4f239c4291":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fe41d88a322417890ab8f7206bb6a8a","IPY_MODEL_ff5968d98f5a4aff9d3c28b24f2a71cc","IPY_MODEL_4dc498f2a6b949018e44eac273f17516"],"layout":"IPY_MODEL_d4b495e9dbcc42dc9a8fd5c4df14f707"}},"1fe41d88a322417890ab8f7206bb6a8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6fd2d0367514cb5a3d8e3cc6404498d","placeholder":"​","style":"IPY_MODEL_8fa69581a2b04e6caa4614475e4cd5e7","value":"Downloading vocab.json: 100%"}},"ff5968d98f5a4aff9d3c28b24f2a71cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92b861d7d2747809d9fbea6ce411b6c","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9361cf30395403cb4d8a88447da745b","value":898823}},"4dc498f2a6b949018e44eac273f17516":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea6864bb85fb4705978dfa416967d969","placeholder":"​","style":"IPY_MODEL_ac88fff024ab45a7a455a79ac139ccc6","value":" 878k/878k [00:00&lt;00:00, 2.04MB/s]"}},"d4b495e9dbcc42dc9a8fd5c4df14f707":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6fd2d0367514cb5a3d8e3cc6404498d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fa69581a2b04e6caa4614475e4cd5e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c92b861d7d2747809d9fbea6ce411b6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9361cf30395403cb4d8a88447da745b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea6864bb85fb4705978dfa416967d969":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac88fff024ab45a7a455a79ac139ccc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32dbe1ba49c641a2affeecfe8c719fc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63f731335c9643f2be75d402e526feb7","IPY_MODEL_b8565fb319564218ac72c3b8decdf163","IPY_MODEL_5d81f7e76a604e62a32fcd85955e0cf3"],"layout":"IPY_MODEL_ead4387536cb442585dd8e5b79e665b5"}},"63f731335c9643f2be75d402e526feb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b96583ce3874ff6af6ae410d9f8cf44","placeholder":"​","style":"IPY_MODEL_5997b9e9e2c8496da00b23d769adc9a3","value":"Downloading merges.txt: 100%"}},"b8565fb319564218ac72c3b8decdf163":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc8e1dfe203d469c8d73ff013a7fc3fa","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fd6b1932bdf45f4be0081bee05eccc1","value":456318}},"5d81f7e76a604e62a32fcd85955e0cf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc10b4291b144d4fabf35c43a2ed3ac5","placeholder":"​","style":"IPY_MODEL_16ec5b9795284590b6e48a919e5b15fb","value":" 446k/446k [00:00&lt;00:00, 789kB/s]"}},"ead4387536cb442585dd8e5b79e665b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b96583ce3874ff6af6ae410d9f8cf44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5997b9e9e2c8496da00b23d769adc9a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc8e1dfe203d469c8d73ff013a7fc3fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fd6b1932bdf45f4be0081bee05eccc1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc10b4291b144d4fabf35c43a2ed3ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16ec5b9795284590b6e48a919e5b15fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e038200102f46749ea557ddd877a504":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c6758b266b64604b9b562d88188bbdb","IPY_MODEL_33219aeb08fa4d8ba3dd95c55514febc","IPY_MODEL_107c6d93640a48d09df2e2d7e27a1178"],"layout":"IPY_MODEL_d8236d969732444bb4093aa9412a09fc"}},"7c6758b266b64604b9b562d88188bbdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8228db0194d43e7b13e8d2e15c7916b","placeholder":"​","style":"IPY_MODEL_accee7998edd4671ac3712038e92d832","value":"Sanity Checking DataLoader 0: 100%"}},"33219aeb08fa4d8ba3dd95c55514febc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf8d5276d8e346f58a164a2d6db756a8","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b66b65118ae4d76966b85a673f4cbbc","value":2}},"107c6d93640a48d09df2e2d7e27a1178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28fe605a88114c89ac83df2a9ce73895","placeholder":"​","style":"IPY_MODEL_c325ed9e974c41d998673fa0cb36130d","value":" 2/2 [00:03&lt;00:00,  1.78s/it]"}},"d8236d969732444bb4093aa9412a09fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b8228db0194d43e7b13e8d2e15c7916b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"accee7998edd4671ac3712038e92d832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf8d5276d8e346f58a164a2d6db756a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b66b65118ae4d76966b85a673f4cbbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28fe605a88114c89ac83df2a9ce73895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c325ed9e974c41d998673fa0cb36130d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8b236e303934db5a64cc9b68a3c3f1f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_377dae63a98a484fa050d11b33d4e148","IPY_MODEL_8577e39e86844a3b8755d0f4acbde109","IPY_MODEL_0555965c88394191b628036f18fe3c55"],"layout":"IPY_MODEL_d299a3d90d254872b2ba2642df5dd2f7"}},"377dae63a98a484fa050d11b33d4e148":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_426fa0b0388943e0acff6fa588f3612a","placeholder":"​","style":"IPY_MODEL_1bbeb08e3837449396d78ac077469b59","value":"Epoch 1: 100%"}},"8577e39e86844a3b8755d0f4acbde109":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c498ad284224e8d8670f8ee0a5027d0","max":3716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e74d6e616b64ce2a2e24f20ede84317","value":3716}},"0555965c88394191b628036f18fe3c55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dbbe82c96d4438aa736f7bfb75e64ec","placeholder":"​","style":"IPY_MODEL_86145f6e8c0a46078e3a397acc098d13","value":" 3716/3716 [34:16&lt;00:00,  1.81it/s, loss=2.27, v_num=0, log_step={&#x27;val_loss&#x27;: 3.0922889709472656, &#x27;val_acc&#x27;: 0.12890625, &#x27;val_rouge1&#x27;: 0.5033037066459656, &#x27;val_rouge2&#x27;: 0.2551927864551544, &#x27;val_rougeL&#x27;: 0.4146365523338318}, log_epoch={&#x27;train_loss&#x27;: 2.533702850341797, &#x27;train_acc&#x27;: 0.1474381685256958, &#x27;train_rouge1&#x27;: 0.45525115728378296, &#x27;train_rouge2&#x27;: 0.24355921149253845, &#x27;train_rougeL&#x27;: 0.36285462975502014}]"}},"d299a3d90d254872b2ba2642df5dd2f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"426fa0b0388943e0acff6fa588f3612a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bbeb08e3837449396d78ac077469b59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c498ad284224e8d8670f8ee0a5027d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e74d6e616b64ce2a2e24f20ede84317":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0dbbe82c96d4438aa736f7bfb75e64ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86145f6e8c0a46078e3a397acc098d13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a02a660df894b17b2fcbb437cccf172":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a39df37e68b84fa584d5b4ed3d15fd86","IPY_MODEL_37f12853faa5449b96680916cd026a11","IPY_MODEL_4eb1591d148d450895cd5cebdc09192d"],"layout":"IPY_MODEL_b98cbbf40ed44609b455a4fedd27b268"}},"a39df37e68b84fa584d5b4ed3d15fd86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00ef499e710420d9db5b284dd381bad","placeholder":"​","style":"IPY_MODEL_52d3ac668b274991b9adeca601056bd4","value":"Validation DataLoader 0: 100%"}},"37f12853faa5449b96680916cd026a11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a68b535ad1f940e2a6ceb4f93ff0f51d","max":413,"min":0,"orientation":"horizontal","style":"IPY_MODEL_10157ce2ae3641a8b523847dc1e66ea9","value":413}},"4eb1591d148d450895cd5cebdc09192d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_810f814b665b4b08a36fe4223a5db73e","placeholder":"​","style":"IPY_MODEL_f88df077b91e47aaa8d069998bd561e7","value":" 413/413 [02:24&lt;00:00,  2.92it/s]"}},"b98cbbf40ed44609b455a4fedd27b268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"a00ef499e710420d9db5b284dd381bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52d3ac668b274991b9adeca601056bd4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a68b535ad1f940e2a6ceb4f93ff0f51d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10157ce2ae3641a8b523847dc1e66ea9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"810f814b665b4b08a36fe4223a5db73e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f88df077b91e47aaa8d069998bd561e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a38ae65ccbdd45b9a52b7b2fd0de06d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed5761ad48eb42b7bdd035c69430e81c","IPY_MODEL_369089b7003142debb92fc4bf88afe87","IPY_MODEL_7f580f986386485b84a3c00c8f04db4f"],"layout":"IPY_MODEL_0126a9adcacb445abf420a8aa31798f6"}},"ed5761ad48eb42b7bdd035c69430e81c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0664ad96062449eacec7db5e3fc2bb8","placeholder":"​","style":"IPY_MODEL_9d3ab6b089064d2b88531db3145cc8bc","value":"Validation DataLoader 0: 100%"}},"369089b7003142debb92fc4bf88afe87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b513a6a13f2c408c8fa83276e9533a35","max":413,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3588cac786e547a988caf9a811f6d6d9","value":413}},"7f580f986386485b84a3c00c8f04db4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea5bcf6412034edeac62c12ddf26fbd2","placeholder":"​","style":"IPY_MODEL_2fd8164f7c12490d89cbc0129e1ec2e8","value":" 413/413 [02:29&lt;00:00,  2.83it/s]"}},"0126a9adcacb445abf420a8aa31798f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"d0664ad96062449eacec7db5e3fc2bb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d3ab6b089064d2b88531db3145cc8bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b513a6a13f2c408c8fa83276e9533a35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3588cac786e547a988caf9a811f6d6d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea5bcf6412034edeac62c12ddf26fbd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fd8164f7c12490d89cbc0129e1ec2e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e508301b35a44f6ae2feb97c5cd8c27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f21d5214d1db423997beed05f258ed82","IPY_MODEL_eb957d335d6a41d1986ae27f20b80340","IPY_MODEL_843f56010ea6460a93ab28971345f00a"],"layout":"IPY_MODEL_0f3118d5574848458d8e554459b41357"}},"f21d5214d1db423997beed05f258ed82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ae25b98fb9c4435a921237c0decccc7","placeholder":"​","style":"IPY_MODEL_75fb360e338d494bb8550d4f38e938b4","value":"Downloading config.json: 100%"}},"eb957d335d6a41d1986ae27f20b80340":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_546115ae0cd543bc83aef1011eb5d220","max":1199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea701a535cfd498696adb90a84e71f84","value":1199}},"843f56010ea6460a93ab28971345f00a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1cbb3580e664314ad5be4419a6ecd5e","placeholder":"​","style":"IPY_MODEL_3d50a661f8624e1083e573a7fe08b0af","value":" 1.17k/1.17k [00:00&lt;00:00, 32.3kB/s]"}},"0f3118d5574848458d8e554459b41357":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ae25b98fb9c4435a921237c0decccc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75fb360e338d494bb8550d4f38e938b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"546115ae0cd543bc83aef1011eb5d220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea701a535cfd498696adb90a84e71f84":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1cbb3580e664314ad5be4419a6ecd5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d50a661f8624e1083e573a7fe08b0af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44473f635bc3419381608f72cda418fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb0ea0e595654a3ead9a311794dea47e","IPY_MODEL_6fe996b860b34909b51bb9210280872b","IPY_MODEL_904aa7e8b0b643ed9f7391f2d1fd30a4"],"layout":"IPY_MODEL_56cea87687ab46dda1b3b9126de1e502"}},"fb0ea0e595654a3ead9a311794dea47e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49cd8cf68c484c5889664210490ae7ad","placeholder":"​","style":"IPY_MODEL_1b8216b98f2d482b92446880a3f3dce0","value":"Downloading pytorch_model.bin: 100%"}},"6fe996b860b34909b51bb9210280872b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46b665a9ad8e4c8d80f072968a107a53","max":891691430,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c90fe3d0ed5f44b48a5e67d0441c955a","value":891691430}},"904aa7e8b0b643ed9f7391f2d1fd30a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c934af66a07e498bba191fa203392da0","placeholder":"​","style":"IPY_MODEL_09b036ce880a4fd1a3d6259d646a63ad","value":" 850M/850M [00:20&lt;00:00, 43.8MB/s]"}},"56cea87687ab46dda1b3b9126de1e502":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49cd8cf68c484c5889664210490ae7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b8216b98f2d482b92446880a3f3dce0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46b665a9ad8e4c8d80f072968a107a53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c90fe3d0ed5f44b48a5e67d0441c955a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c934af66a07e498bba191fa203392da0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b036ce880a4fd1a3d6259d646a63ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cba037101ff1465396efb9c48f7d58a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66a956396f3e408abd89de8310be20d0","IPY_MODEL_1cd1e4850ecf4e359b5074616ec42eea","IPY_MODEL_ad05445d641e4035894ff3400e1d7db0"],"layout":"IPY_MODEL_15763b0d027e4d8fbee270cdbf50b040"}},"66a956396f3e408abd89de8310be20d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e32f2380b5244e548bf4110872a5f533","placeholder":"​","style":"IPY_MODEL_38356a838dfe4fd78691b414985af781","value":"Downloading spiece.model: 100%"}},"1cd1e4850ecf4e359b5074616ec42eea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d25e23e400c14527b25274b56dcda70b","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c434b3d44484f1aabbb7b36aaeec64b","value":791656}},"ad05445d641e4035894ff3400e1d7db0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47194f8c1d1f4a36af4dc920cb43a2c1","placeholder":"​","style":"IPY_MODEL_d4e71e9bff6340e4b981a4ed202bd1da","value":" 773k/773k [00:00&lt;00:00, 1.83MB/s]"}},"15763b0d027e4d8fbee270cdbf50b040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e32f2380b5244e548bf4110872a5f533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38356a838dfe4fd78691b414985af781":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d25e23e400c14527b25274b56dcda70b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c434b3d44484f1aabbb7b36aaeec64b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47194f8c1d1f4a36af4dc920cb43a2c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4e71e9bff6340e4b981a4ed202bd1da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc36d28bfc794d7dbeb9a6e8a317bacc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82d2f9b2003648e1afc82e71c63fdbc5","IPY_MODEL_c3e829476ef74217994c535db08c1b0d","IPY_MODEL_be0ec10779394df78ffde554a609518c"],"layout":"IPY_MODEL_6cbad8ac4d0442f896749d9fec825f06"}},"82d2f9b2003648e1afc82e71c63fdbc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79201ff1decd434caaf6f7e4f9b1b9bf","placeholder":"​","style":"IPY_MODEL_6c024ae1a6974760836ec9e4fe074463","value":"Sanity Checking DataLoader 0: 100%"}},"c3e829476ef74217994c535db08c1b0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0bc06e44c6e54cc2a6b3aae1634687f5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_633ef215bd7445169dede22d9656b7b7","value":2}},"be0ec10779394df78ffde554a609518c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0418a7df5f5d4fba8a69c44a15593b6e","placeholder":"​","style":"IPY_MODEL_13e04c06b4ab4bd88a44b19987f57536","value":" 2/2 [00:03&lt;00:00,  1.79s/it]"}},"6cbad8ac4d0442f896749d9fec825f06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"79201ff1decd434caaf6f7e4f9b1b9bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c024ae1a6974760836ec9e4fe074463":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0bc06e44c6e54cc2a6b3aae1634687f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"633ef215bd7445169dede22d9656b7b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0418a7df5f5d4fba8a69c44a15593b6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e04c06b4ab4bd88a44b19987f57536":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04db8b4ccbc44d56a22b2ef1671acff9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3a44a336a764a13a9cea52e6fb96b26","IPY_MODEL_d9a05faffad742efbf1e3bdecbdca6ad","IPY_MODEL_cf913c363b3e4f43b7181aa177f457a3"],"layout":"IPY_MODEL_c8580717b95c4f39a521b2f83d4908c8"}},"c3a44a336a764a13a9cea52e6fb96b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8be6459810b43cab66622902702a899","placeholder":"​","style":"IPY_MODEL_53a8fb902d8e407ba9e395a9dc3c11ea","value":"Epoch 1:  17%"}},"d9a05faffad742efbf1e3bdecbdca6ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee2fea5e467b4b528a84c69f6e5cb14a","max":3716,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a60c97445f9402099004d9dbdceef39","value":640}},"cf913c363b3e4f43b7181aa177f457a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f72434aa3c7047e3b9a02736940a459e","placeholder":"​","style":"IPY_MODEL_14e54f56cbdc4eca8d7b3461f036f6fc","value":" 640/3716 [25:01&lt;2:00:16,  2.35s/it, loss=2.4, v_num=1, log_step={&#x27;train_loss&#x27;: 3.2449021339416504, &#x27;train_acc&#x27;: 0.52197265625, &#x27;train_rouge1&#x27;: 0.5130704641342163, &#x27;train_rouge2&#x27;: 0.2769645154476166, &#x27;train_rougeL&#x27;: 0.44436103105545044}, log_epoch={&#x27;train_loss&#x27;: 2.6227786540985107, &#x27;train_acc&#x27;: 0.44398391246795654, &#x27;train_rouge1&#x27;: 0.503111720085144, &#x27;train_rouge2&#x27;: 0.25850364565849304, &#x27;train_rougeL&#x27;: 0.42919665575027466}]"}},"c8580717b95c4f39a521b2f83d4908c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"b8be6459810b43cab66622902702a899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53a8fb902d8e407ba9e395a9dc3c11ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2fea5e467b4b528a84c69f6e5cb14a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a60c97445f9402099004d9dbdceef39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f72434aa3c7047e3b9a02736940a459e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14e54f56cbdc4eca8d7b3461f036f6fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48dd71a1d110434f833c0d820f0acc87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12cbaa1f3b8047d599490de80c5a245f","IPY_MODEL_a12a5a47cec9492eb9ca7a999a9630f1","IPY_MODEL_b33cebf0e921421eabbfc3b2a9eed147"],"layout":"IPY_MODEL_fd0626a9068b41baa5ffd7fca103c3d4"}},"12cbaa1f3b8047d599490de80c5a245f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df6b3d0aa6c94603b726e4ae2391d7a2","placeholder":"​","style":"IPY_MODEL_e22ff3611df24f449c973fcecbbadf6b","value":"Validation DataLoader 0: 100%"}},"a12a5a47cec9492eb9ca7a999a9630f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_41e88a638e4245e4924e29c26530c8db","max":413,"min":0,"orientation":"horizontal","style":"IPY_MODEL_888302c78468498ea4ea7e1e7e020047","value":413}},"b33cebf0e921421eabbfc3b2a9eed147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35f230da920d4644a5392b6ef034e652","placeholder":"​","style":"IPY_MODEL_63692891b6274e42a48f917a3fde3eef","value":" 413/413 [12:38&lt;00:00,  1.81s/it]"}},"fd0626a9068b41baa5ffd7fca103c3d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"df6b3d0aa6c94603b726e4ae2391d7a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22ff3611df24f449c973fcecbbadf6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41e88a638e4245e4924e29c26530c8db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"888302c78468498ea4ea7e1e7e020047":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35f230da920d4644a5392b6ef034e652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63692891b6274e42a48f917a3fde3eef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"wMh1bopaHl4E","colab":{"base_uri":"https://localhost:8080/"},"outputId":"61780a75-e80e-4279-e3e4-268f4e607cfe","executionInfo":{"status":"ok","timestamp":1659241033982,"user_tz":420,"elapsed":1965,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive \n","from google.colab import files\n","\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"code","source":["!pip install gensim==3.8.3 --quiet\n","\n","!pip install tensorflow-datasets --quiet\n","!pip install -U tensorflow-text==2.8.2 --quiet\n","!pip install pydot --quiet\n","!pip install -q sentencepiece\n","# !pip install -q transformers\n","# !pip install torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0\n","!pip install transformers\n","!pip install pytorch_lightning\n","# !pip install rouge-score"],"metadata":{"id":"VJ9pk1FNHtUH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cae326a9-c6a0-44bb-810e-067ca386f3c2","executionInfo":{"status":"ok","timestamp":1659241058723,"user_tz":420,"elapsed":24761,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.7/dist-packages (1.6.5)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.0+cu113)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n","Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.3.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.9.3)\n","Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.17.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.7.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.2.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.4.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.47.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n"]}]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"6N4P88SRH_Mz"}},{"cell_type":"code","source":["\n","import numpy as np\n","import pandas as pd\n","from collections import Counter, OrderedDict\n","\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.layers import Embedding, Input, Dense, Lambda\n","from tensorflow.keras.models import Model\n","import tensorflow.keras.backend as K\n","import tensorflow_datasets as tfds\n","import tensorflow_text as tf_text\n","\n","\n","import sklearn as sk\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.data import find\n","\n","import matplotlib.pyplot as plt\n","\n","import re\n","import yaml\n","\n","#This continues to work with gensim 3.8.3.  It doesn't yet work with 4.x.  \n","#Make sure your pip install command specifies gensim==3.8.3\n","import gensim\n","\n","import numpy as np\n","\n","from transformers import BertTokenizer, TFBertModel, T5Tokenizer, TFT5ForConditionalGeneration\n","from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n","from torchmetrics.text.rouge import ROUGEScore\n","\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","from pprint import pprint\n","\n","from pathlib import Path\n","import logging\n","import time\n","from logging.handlers import RotatingFileHandler\n","\n","\n","\n","import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","import math\n","\n","# import nltk\n","# nltk.download('punkt')\n","# from nltk.tokenize import sent_tokenize\n","import transformers\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import torchmetrics\n","import tensorboard\n","from tensorboard.backend.event_processing import event_accumulator\n","\n","\n","\n","# device = 'cuda' if cuda.is_available() else 'cpu'\n","import pytorch_lightning as pl\n","from pytorch_lightning import loggers as pl_loggers\n","\n","\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup,  BartTokenizer, BartForConditionalGeneration,  BartConfig\n",")\n","\n","\n","import gc\n","import string\n","import re\n","from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n","\n","\n"],"metadata":{"id":"qd5nroGKH50W","executionInfo":{"status":"ok","timestamp":1659241086147,"user_tz":420,"elapsed":20397,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"STTb6UwoayBF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find index of \"chorus\" or \"hook\" and iterate till you hit 1st double new line .\n","\n","# then iterate through number of counts of chorus and replace it with <extra_id_i>. Get tuple of start and end indices of each chorus.\n","\n","# This will help us find start and end indices of each verse\n","\n"," "],"metadata":{"id":"1yKd1-iuJGGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# then clean strings \n","# remove chorus and verse titles\n","\n","# keep new lines in there. Just remove double new lines\n","# keep punctuation in there for now\n","\n","# clean the text\n","def preprocess(sentence):\n","    sentence=str(sentence)\n","\n","    #uncased means words are all lowercased\n","    sentence = sentence.lower()\n","\n","    # remove chorus and verse titles\n","    # remove the brackets and everything in the brackets\n","    sentence = re.sub(\"[\\[].*?[\\]]\", \"\", sentence)\n","   \n","    # replace parentheses only\n","    sentence = re.sub(\"[\\(\\)]\", \"\", sentence)\n","\n","    sentence = sentence.replace(\"\\n\", \" \")\n","\n","    # remove double new lines with single new line. So there is a new line space between sections\n","    # sentence= re.sub(\"(\\n\\n)\", \"\\n\", sentence)\n","\n","    return sentence\n","\n","\n","# p = preprocess(verse_masked_lyrics)\n","# print(p)\n","\n","# cleantrain_texts = list(map(preprocess, train_texts))\n","\n","# #you need to make sure you apply the same preprocessing to the test and validation sets\n","\n","# ### YOUR CODE HERE\n","\n","# cleantest_texts = list(map(preprocess, test_texts))\n","# cleanvalid_texts = list(map(preprocess, valid_texts))\n","# ### END YOUR CODE"],"metadata":{"id":"ba0R6uRbeTot","executionInfo":{"status":"ok","timestamp":1659241086150,"user_tz":420,"elapsed":16,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# BART "],"metadata":{"id":"c1XaEUPLtqVX"}},{"cell_type":"markdown","source":["https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb#scrollTo=HS8mNXq6bdxq"],"metadata":{"id":"wxZ-aqqMt-_n"}},{"cell_type":"markdown","source":["https://github.com/Lightning-AI/lightning/discussions/11770\n","\n","https://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/\n"],"metadata":{"id":"MQ2PzmRiFbEh"}},{"cell_type":"code","source":[""],"metadata":{"id":"CsZOXH2SYYcr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"metadata":{"id":"TrycPae7OiVo","executionInfo":{"status":"ok","timestamp":1659225340871,"user_tz":420,"elapsed":246,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class BartFineTuner(pl.LightningModule):\n","  def __init__(self, hparams):\n","    super(BartFineTuner, self).__init__()\n","    # self.hparams = hparams\n","    self.hparams.update(vars(hparams))\n","    self.save_hyperparameters()\n","    \n","    self.model = BartForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","    self.tokenizer = BartTokenizer.from_pretrained(hparams.tokenizer_name_or_path, model_max_length = hparams.max_seq_length )\n","    self.acc = torchmetrics.Accuracy(mdmc_average = 'samplewise')\n","    self.scorer = ROUGEScore() #rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n","  \n","  def is_logger(self):\n","    return self.trainer.global_rank <= 0\n","  \n","  def forward(\n","      # doesn't have a max_length method, so it will keep trying to predict the same length as the target ids\n","      self, input_ids, attention_mask=None, decoder_attention_mask=None, labels=None\n","  ):\n","    return self.model(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        # decoder_input_ids=decoder_input_ids,\n","        decoder_attention_mask=decoder_attention_mask,\n","        labels=labels,\n","    )\n","\n","  def _step(self, batch):\n","    labels = batch[\"target_ids\"]\n","    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","    outputs = self(\n","        input_ids=batch[\"source_ids\"],\n","        attention_mask=batch[\"source_mask\"],\n","        # takes in labels and shifts right to create decoder input ids shift\n","        labels=labels,\n","        decoder_attention_mask=batch['target_mask']\n","    )\n","\n","    loss = outputs[0]\n","    # print(\"Output keys: \", outputs.keys())\n","    # print(\"Batch: \" , batch)\n","    # print(\"Batch shape :\" , batch.shape)\n","\n","\n","    # source = batch['source_ids']\n","\n","    #  Pred Output size :  torch.Size([8, 256, 32128])\n","    # Target Labels size :  torch.Size([8, 256])\n","\n","\n","    # values are the probabilities.\n","    # indices are the indices\n","    # y_pred = y_max_idx  # predictions are really the inx \\in [n_classes] with the highest scores\n","    \n","    \n","\n","\n","  \n","    \n","    # print(\"******\")\n","    # print(\"Pred Output size : \", preds.size())\n","    # print(\"Target Labels size : \", torch.max(labels, torch.tensor([0])).size())\n","\n","\n","    pred_ids = torch.max(outputs['logits'], 2).indices\n","\n","    # change pred ids index of blank string token at end to index of PAD token, like target label has\n","    # go backward\n","    # DON\"T NEED FOR BART\n","    # for i in range(pred_ids.size(0)):\n","    #   for j in range(1, pred_ids.size(1) + 1):\n","    #     # if  value is  3, then replace current valuw ith 0 and break\n","    #     # print(k[i][j])\n","    #     if pred_ids[i][-j] == 3:\n","    #       pred_ids[i][-j]=0\n","    #     else:\n","    #       break\n","  \n","\n","    # change all the empty strings (word ind = 3) at the end to 0's to represent PAD token ind.\n","    # for each tensor in pred_ids, maks only \n","\n","    pred_str_li = self.tokenizer.batch_decode(pred_ids,skip_special_tokens=True, clean_up_tokenization_spaces=False)\n","    \n","    # get the values before the cast. set -100 to 1 . Pad token or 1 is the same, it will be ignored in string\n","    target_ids = torch.max(batch[\"target_ids\"], torch.tensor([1]).cuda())\n","    # print(\"target_ids : \", target_ids)\n","    # print(\"pred_ids : \", pred_ids)\n","\n","\n","\n","    # print(\"Tokenizer max length \", self.tokenizer.model_max_length)\n","\n","    # print(\"target_ids arr : \", target_ids.shape)\n","    # print(\"pred_ids arr : \", pred_ids.shape)\n","\n","\n","\n","    target_str_li = self.tokenizer.batch_decode(target_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n"," \n","    \n","    # predicts what should be in the mask\n","    \n","    # get rogue scores for each string pair and avergae it\n","    \n","    # rogue score for bathc is average of that batch, loss laredy calculatd that average\n","\n","    # rouge(preds, target)\n","\n","    rouge_scores = self.scorer(pred_str_li, target_str_li)\n","    # torch.mean(torch.Tensor([self.scorer.score(target_str_li[i], pred_str_li[i])['rouge1'] for i in range(len(pred_str_li))]))\n","    #get f1 score of everything\n","\n","    rouge1 = rouge_scores['rouge1_fmeasure']\n","    rouge2 = rouge_scores['rouge2_fmeasure']\n","    rougeL = rouge_scores['rougeL_fmeasure']\n","    # rougeLsum = rouge_scores['rougeLsum_fmeasure']\n","\n","  \n","\n","    preds_new =  torch.max(pred_ids, torch.tensor([1]).cuda())\n","    labels_new = torch.max(labels, torch.tensor([1]).cuda())\n","\n","    # set -100 to 0's\n","    acc = self.acc(preds_new, labels_new)\n","    # self.log('training/step_acc', acc)\n","    # print(\"Target STR : \", target_str_li)\n","    # print(\"Pred   STR : \", pred_str_li)\n","\n","    # print(\"Target FULL : \", self.tokenizer.batch_decode(target_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False))\n","    # print(\"Pred   FULL : \", self.tokenizer.batch_decode(pred_ids,skip_special_tokens=False, clean_up_tokenization_spaces=False))\n","    \n","    # print(\"LOSS : \" +  str(loss.item()) + \" , ACC: \" + str(acc.item())  +\" , ROUGE 1: \" + str(rouge1.item())  + \" , ROUGE 2: \" + str(rouge2.item()) +  \" , ROUGE L: \" + str(rougeL.item())  )\n","   \n","\n","    return loss, acc, rouge1, rouge2, rougeL\n","\n","  def training_step(self, batch, batch_idx):\n","    loss, acc, rouge1, rouge2, rougeL = self._step(batch)\n","\n","    tensorboard_logs = {\"train_loss\": loss, \"train_acc\": acc, \"train_rouge1\": rouge1, \"train_rouge2\": rouge2, \"train_rougeL\": rougeL  }\n","\n","    self.log('log', tensorboard_logs,  prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","\n","    # print(\"TRAIN STEP: \", tensorboard_logs)\n","\n","    \n","    return {\"loss\": loss, \"acc\": acc,   \"rouge1\": rouge1, \"rouge2\": rouge2,  \"rougeL\": rougeL, \"log\": tensorboard_logs}\n","  \n","  def training_epoch_end(self, outputs):\n","    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","    avg_train_acc = torch.stack([x[\"acc\"] for x in outputs]).mean()\n","    avg_train_rouge1 = torch.stack([x[\"rouge1\"] for x in outputs]).mean()\n","    avg_train_rouge2 = torch.stack([x[\"rouge2\"] for x in outputs]).mean()\n","    avg_train_rougeL = torch.stack([x[\"rougeL\"] for x in outputs]).mean()\n","\n","    tensorboard_logs = {\"train_loss\": avg_train_loss, \"train_acc\": avg_train_acc, \"train_rouge1\": avg_train_rouge1, \"train_rouge2\": avg_train_rouge2, \"train_rougeL\": avg_train_rougeL }\n","\n","    self.log('step', self.trainer.current_epoch)\n","    self.log('avg_train_loss', avg_train_loss)\n","    self.log('avg_train_acc', avg_train_acc)\n","    \n","    self.log('avg_train_rouge1', avg_train_rouge1)\n","    self.log('avg_train_rouge2', avg_train_rouge2)\n","    self.log('avg_train_rougeL', avg_train_rougeL)\n","    self.log('log', tensorboard_logs)\n","    self.log('progress_bar', tensorboard_logs)\n","\n"," \n","    print(\"TRAIN Epoch End: \", tensorboard_logs)\n","\n","    # return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","  def validation_step(self, batch, batch_idx):\n","    loss, acc, rouge1, rouge2, rougeL = self._step(batch)\n","    tensorboard_logs = {\"val_loss\": loss, \"val_acc\": acc, \"val_rouge1\": rouge1, \"val_rouge2\": rouge2, \"val_rougeL\": rougeL}\n","\n","    # print(\"VAL STEP: \", {tensorboard_logs)\n","    self.log('log', tensorboard_logs,  prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","    return {\"val_loss\": loss, \"val_acc\": acc, \"val_rouge1\": rouge1, \"val_rouge2\": rouge2, \"val_rougeL\": rougeL}\n","\n","    \n","  def validation_epoch_end(self, outputs):\n","    \n","    avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","    avg_val_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n","    avg_val_rouge1 = torch.stack([x[\"val_rouge1\"] for x in outputs]).mean()\n","    avg_val_rouge2 = torch.stack([x[\"val_rouge2\"] for x in outputs]).mean()\n","    avg_val_rougeL = torch.stack([x[\"val_rougeL\"] for x in outputs]).mean()\n","\n","    # print(\"VAL EPOCH END  Keys: \", outputs.keys() )\n","    \n","    tensorboard_logs = {\"val_loss\": avg_val_loss, \"val_acc\": avg_val_acc, \"val_rouge1\": avg_val_rouge1, \"val_rouge2\": avg_val_rouge2, \"val_rougeL\": avg_val_rougeL}\n","    # return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    self.log('avg_val_loss', avg_val_loss)\n","    self.log('avg_val_acc', avg_val_acc)\n","    self.log('avg_val_rouge1', avg_val_rouge1)\n","    self.log('avg_val_rouge2', avg_val_rouge2)\n","    self.log('avg_val_rougeL', avg_val_rougeL)\n","\n","    self.log('log', tensorboard_logs)\n","    self.log('progress_bar', tensorboard_logs)\n","\n","    print(\"VAL Epoch End: \", tensorboard_logs)\n","\n","\n","  def configure_optimizers(self):\n","    \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","    model = self.model\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": self.hparams.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","    self.opt = optimizer\n","    return [optimizer]\n","  \n","  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure=None, \n","                    on_tpu=False, using_native_amp=False, \n","                    using_lbfgs=False):\n","    # if on_tpu == True: #self.trainer.use_tpu:\n","    #   # xm.optimizer_step(optimizer)\n","    #   optimizer.step()\n","    # else:\n","    # optimizer.step(closure=optimizer_closure)\n","    optimizer.step(closure=optimizer_closure)\n","    optimizer.zero_grad()\n","    self.lr_scheduler.step()\n","  \n","  def get_tqdm_dict(self):\n","    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","    return tqdm_dict\n","\n","  def train_dataloader(self):\n","    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams, col = \"_b\")\n","    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=False, shuffle=False, num_workers=4)\n","    t_total = (\n","        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","        // self.hparams.gradient_accumulation_steps\n","        * float(self.hparams.num_train_epochs)\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","    )\n","    self.lr_scheduler = scheduler\n","    return dataloader\n","\n","  def val_dataloader(self):\n","    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams, col = \"_b\")\n","    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"],"metadata":{"id":"sNK3nEpRwDoS","executionInfo":{"status":"ok","timestamp":1659225347652,"user_tz":420,"elapsed":477,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FeydzTqHMSrF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)\n","\n","\n","\n","class LoggingCallback(pl.Callback):\n","  def on_validation_end(self, trainer, pl_module):\n","    logger.info(\"***** Validation results *****\")\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","      # Log results\n","      output_val_results_file = os.path.join(pl_module.hparams.output_dir, \"val_results.txt\")\n","      with open(output_val_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          # if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","          writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","  def on_test_end(self, trainer, pl_module):\n","    logger.info(\"***** Test results *****\")\n","\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","      with open(output_test_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          #if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","          writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"],"metadata":{"id":"CNT-0YOYuQKh","executionInfo":{"status":"ok","timestamp":1659225358364,"user_tz":420,"elapsed":242,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oF1XGs7MXt0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bart_args_dict = dict(\n","    data_dir=\"./gdrive/MyDrive/W266 Final Project 2/data/new_data\",  #directory for all the input data files # path for data files\n","    output_dir=\"./gdrive/MyDrive/W266 Final Project 2/models/BART_base-1/\", # path to save the checkpoints\n","    model_name_or_path='facebook/bart-base',\n","    tokenizer_name_or_path='facebook/bart-base',\n","    max_seq_length=256,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=8,\n","    eval_batch_size=8,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=16,\n","    n_gpu= 1,#1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    amp_backend = \"apex\",\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")"],"metadata":{"id":"P43NOLuZuc4x","executionInfo":{"status":"ok","timestamp":1659225365598,"user_tz":420,"elapsed":234,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', model_max_length = args_dict['max_seq_length'])"],"metadata":{"id":"yFxlbME2uc7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model will take in only 256 tokens max. During trianing, calling model to get prediciotns will also produce 256 tokens\n","# only generate function() takes in variable token length to define how many words the model should generate\n","# tokenizer.model_max_length"],"metadata":{"id":"sdWgL2SlUjTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CTbesaGMPvhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class SongDataset(Dataset):\n","\n","# class ImdbDataset(Dataset):\n","  def __init__(self, tokenizer, data_dir,  type_path, max_len=256, col = \"\"):\n","\n","    self.file_path = os.path.join(data_dir, type_path)\n","    self.file = self.file_path + \".csv\"\n","    \n","    self.max_len = max_len\n","    self.tokenizer = tokenizer\n","    self.inputs = []\n","    self.targets = []\n","    self.col = col\n","\n","    self._build()\n","  \n","  def __len__(self):\n","    return len(self.inputs)\n","  \n","  def __getitem__(self, index):\n","    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","    target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","    # gets it into 1 dimension\n","    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","  \n","  def _build(self):\n","    self._build_examples_from_files(self.file)\n","\n","  \n","  def _build_examples_from_files(self, file):\n","\n","    # read in 1 sample\n","    # read in 1 input at a time\n","\n","    # ierate through df rows\n","\n","    text_df = pd.read_csv(self.file)\n","    # print(\"text_df : \", text_df.shape)\n","\n","    for index, row  in text_df.iterrows():\n","      # clean strings here\n","      #input b for BART\n","      input = preprocess(row['inputs' + self.col])\n","      target = preprocess(row['targets' + self.col])\n","      \n","       # tokenize inputs\n","      tokenized_inputs = self.tokenizer.batch_encode_plus(\n","          [input], max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors=\"pt\"\n","      )\n","\n","      tokenized_targets = self.tokenizer.batch_encode_plus(\n","          [target], max_length=self.max_len, padding = 'max_length', truncation=True, return_tensors=\"pt\"\n","      )\n","\n","      self.inputs.append(tokenized_inputs)\n","      self.targets.append(tokenized_targets)\n","\n"],"metadata":{"id":"sCgVQO3HuQQe","executionInfo":{"status":"ok","timestamp":1659241456164,"user_tz":420,"elapsed":17,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CF5DOoQ1nuzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset = SongDataset(tokenizer, args_dict['data_dir'], 'train',  max_len=256)\n","# ImdbDataset(tokenizer, 'aclImdb', 'val',  max_len=512)\n","# len(dataset)"],"metadata":{"id":"BbsRcZVhMKR4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data = dataset[15]\n","# print(tokenizer.decode(data['source_ids']))\n","# print(tokenizer.decode(data['target_ids']))"],"metadata":{"id":"97GQtHjFuoOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ehpOue-QuoTc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BART Train"],"metadata":{"id":"dHF-UNxNuwCN"}},{"cell_type":"code","source":[""],"metadata":{"id":"wk3qBkiaMTSL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# args_dict.update({'data_dir': 'T5', 'output_dir': 'outputs', 'num_train_epochs':2})\n","bart_args = argparse.Namespace(**bart_args_dict)\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=bart_args.output_dir, filename='{epoch}-{val_loss:.2f}', monitor=\"avg_val_loss\", mode=\"min\", save_top_k=5\n",")\n","\n","# path to save lightning_logs\n","tb_logger = pl_loggers.TensorBoardLogger(save_dir=bart_args.output_dir)\n","\n","\n","bart_train_params = dict(\n","    logger = tb_logger,\n","    \n","    accumulate_grad_batches=bart_args.gradient_accumulation_steps,\n","\n","\n","    gpus=bart_args.n_gpu, \n","    # accelerator = \"cpu\",\n","\n","    max_epochs=bart_args.num_train_epochs,\n","    log_every_n_steps = 50, # change based on size of dataset\n","    # early_stop_callback=False,\n","    precision= 16 if bart_args.fp_16 else 32,\n","    amp_level=bart_args.opt_level,\n","    amp_backend = bart_args.amp_backend,\n","    gradient_clip_val=bart_args.max_grad_norm,\n","    enable_checkpointing = True,\n","    # checkpoint_callback=checkpoint_callback,\n","    callbacks=[checkpoint_callback, LoggingCallback()],\n",")\n","\n"],"metadata":{"id":"5jgX4OjIuoWh","executionInfo":{"status":"ok","timestamp":1659225413679,"user_tz":420,"elapsed":908,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_dataset(tokenizer, type_path, args, col):\n","  return SongDataset(tokenizer=tokenizer, data_dir=args.data_dir,  type_path=type_path, max_len=args.max_seq_length, col = col)"],"metadata":{"id":"Axj37P7AuoZH","executionInfo":{"status":"ok","timestamp":1659241443174,"user_tz":420,"elapsed":280,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"1soFQG2LcZEU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune Model"],"metadata":{"id":"mhRxzXMjcZpf"}},{"cell_type":"code","source":["bart_model = BartFineTuner(bart_args)"],"metadata":{"id":"KkE3RkmvuoeY","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["79f0508db1f8452aa9a6a5d23ba479ac","42df603915ba4d4092a0f55122ddc820","37c5898ba5304b808a04f3970d5a3f7b","e5b5202f237842bb8f85d5ab2d371e1c","3e3b5a12d1604c688dbdaffc5d69c35a","686616f3f2594672bc9205a92395d946","dd5fdf42f5364e7190195d0b7b9b7a9e","884d9d0c7b9642b5bbf9de6ff553c71f","8127cf98d2684d6896dafc2eb8165ae5","3ea6bc532fd24061b7dead3c3b20ebf3","ba70f190eb524f0e959fe6209ccfed16","4149f29b603f45e39c3594a13d27f998","da598b51787448fdae2982ae8efa287a","1933f8962efe495ab22bc52bf070b4d0","75b1251c04e54907832cd7d015747e83","af1473e3dc3c42a09946b51a95cd45cd","106e6d4fed904026b66de0645ad18e11","670e9a01c632436283649f44aa8423ad","63305387d33340998a9e71171d4e5644","abfb82a045e54cb4b6eef29e4cefd0c4","ba55ff41c43643e08f6b9e19709c2cd6","0c7a1792669f41cd860c82e8f4035fad","8416b6d82fe44ae0b3af2b4f239c4291","1fe41d88a322417890ab8f7206bb6a8a","ff5968d98f5a4aff9d3c28b24f2a71cc","4dc498f2a6b949018e44eac273f17516","d4b495e9dbcc42dc9a8fd5c4df14f707","a6fd2d0367514cb5a3d8e3cc6404498d","8fa69581a2b04e6caa4614475e4cd5e7","c92b861d7d2747809d9fbea6ce411b6c","f9361cf30395403cb4d8a88447da745b","ea6864bb85fb4705978dfa416967d969","ac88fff024ab45a7a455a79ac139ccc6","32dbe1ba49c641a2affeecfe8c719fc0","63f731335c9643f2be75d402e526feb7","b8565fb319564218ac72c3b8decdf163","5d81f7e76a604e62a32fcd85955e0cf3","ead4387536cb442585dd8e5b79e665b5","7b96583ce3874ff6af6ae410d9f8cf44","5997b9e9e2c8496da00b23d769adc9a3","dc8e1dfe203d469c8d73ff013a7fc3fa","4fd6b1932bdf45f4be0081bee05eccc1","fc10b4291b144d4fabf35c43a2ed3ac5","16ec5b9795284590b6e48a919e5b15fb"]},"executionInfo":{"status":"ok","timestamp":1659225501357,"user_tz":420,"elapsed":17133,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}},"outputId":"d070964e-0c12-45d6-cb92-d8ccc3af566e"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79f0508db1f8452aa9a6a5d23ba479ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4149f29b603f45e39c3594a13d27f998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8416b6d82fe44ae0b3af2b4f239c4291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32dbe1ba49c641a2affeecfe8c719fc0"}},"metadata":{}}]},{"cell_type":"code","source":["# REMOVE CPU here\n","bart_trainer = pl.Trainer(**bart_train_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KbnFyYyxu0sJ","outputId":"1c379f88-d2ca-42bb-8142-5e42784b1b4a","executionInfo":{"status":"ok","timestamp":1659225544784,"user_tz":420,"elapsed":257,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"xLrs8Puzjaq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # # clear torch cache\n","\n","gc.collect()\n","\n","torch.cuda.empty_cache()\n","\n"],"metadata":{"id":"ec1xjf1tvFGJ","executionInfo":{"status":"ok","timestamp":1659225558175,"user_tz":420,"elapsed":277,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"flGHMn_5yno0","executionInfo":{"status":"ok","timestamp":1659225553248,"user_tz":420,"elapsed":270,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# start fine-tuning\n","bart_trainer.fit(bart_model)"],"metadata":{"id":"lp_gPNowu5Ok","colab":{"base_uri":"https://localhost:8080/","height":639,"referenced_widgets":["9e038200102f46749ea557ddd877a504","7c6758b266b64604b9b562d88188bbdb","33219aeb08fa4d8ba3dd95c55514febc","107c6d93640a48d09df2e2d7e27a1178","d8236d969732444bb4093aa9412a09fc","b8228db0194d43e7b13e8d2e15c7916b","accee7998edd4671ac3712038e92d832","bf8d5276d8e346f58a164a2d6db756a8","6b66b65118ae4d76966b85a673f4cbbc","28fe605a88114c89ac83df2a9ce73895","c325ed9e974c41d998673fa0cb36130d","c8b236e303934db5a64cc9b68a3c3f1f","377dae63a98a484fa050d11b33d4e148","8577e39e86844a3b8755d0f4acbde109","0555965c88394191b628036f18fe3c55","d299a3d90d254872b2ba2642df5dd2f7","426fa0b0388943e0acff6fa588f3612a","1bbeb08e3837449396d78ac077469b59","3c498ad284224e8d8670f8ee0a5027d0","8e74d6e616b64ce2a2e24f20ede84317","0dbbe82c96d4438aa736f7bfb75e64ec","86145f6e8c0a46078e3a397acc098d13","9a02a660df894b17b2fcbb437cccf172","a39df37e68b84fa584d5b4ed3d15fd86","37f12853faa5449b96680916cd026a11","4eb1591d148d450895cd5cebdc09192d","b98cbbf40ed44609b455a4fedd27b268","a00ef499e710420d9db5b284dd381bad","52d3ac668b274991b9adeca601056bd4","a68b535ad1f940e2a6ceb4f93ff0f51d","10157ce2ae3641a8b523847dc1e66ea9","810f814b665b4b08a36fe4223a5db73e","f88df077b91e47aaa8d069998bd561e7","a38ae65ccbdd45b9a52b7b2fd0de06d9","ed5761ad48eb42b7bdd035c69430e81c","369089b7003142debb92fc4bf88afe87","7f580f986386485b84a3c00c8f04db4f","0126a9adcacb445abf420a8aa31798f6","d0664ad96062449eacec7db5e3fc2bb8","9d3ab6b089064d2b88531db3145cc8bc","b513a6a13f2c408c8fa83276e9533a35","3588cac786e547a988caf9a811f6d6d9","ea5bcf6412034edeac62c12ddf26fbd2","2fd8164f7c12490d89cbc0129e1ec2e8"]},"outputId":"cfcd5c90-43b3-48ea-9a7b-c544745c1130","executionInfo":{"status":"ok","timestamp":1659229748258,"user_tz":420,"elapsed":4179530,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:118: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n","  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n","Missing logger folder: ./gdrive/MyDrive/W266 Final Project 2/models/BART_base-1/lightning_logs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /content/gdrive/.shortcut-targets-by-id/1tdlBf0ibenbP95uaYV-bH3xJJIgmACVX/W266 Final Project 2/models/BART_base-1 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","\n","  | Name   | Type                         | Params\n","--------------------------------------------------------\n","0 | model  | BartForConditionalGeneration | 139 M \n","1 | acc    | Accuracy                     | 0     \n","2 | scorer | ROUGEScore                   | 0     \n","--------------------------------------------------------\n","139 M     Trainable params\n","0         Non-trainable params\n","139 M     Total params\n","557.682   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e038200102f46749ea557ddd877a504"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["VAL Epoch End:  {'val_loss': tensor(3.6333, device='cuda:0'), 'val_acc': tensor(0.1196, device='cuda:0'), 'val_rouge1': tensor(0.3389), 'val_rouge2': tensor(0.1794), 'val_rougeL': tensor(0.2841)}\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8b236e303934db5a64cc9b68a3c3f1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a02a660df894b17b2fcbb437cccf172"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:287: UserWarning: The ``compute`` method of metric _ResultMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n","  f\"The ``compute`` method of metric {self.__class__.__name__}\"\n"]},{"output_type":"stream","name":"stdout","text":["VAL Epoch End:  {'val_loss': tensor(2.5651, device='cuda:0'), 'val_acc': tensor(0.1494, device='cuda:0'), 'val_rouge1': tensor(0.5379), 'val_rouge2': tensor(0.3103), 'val_rougeL': tensor(0.4414)}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('step', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n"]},{"output_type":"stream","name":"stdout","text":["TRAIN Epoch End:  {'train_loss': tensor(0.1926, device='cuda:0'), 'train_acc': tensor(0.1299, device='cuda:0'), 'train_rouge1': tensor(0.4299), 'train_rouge2': tensor(0.2057), 'train_rougeL': tensor(0.3375)}\n"]},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a38ae65ccbdd45b9a52b7b2fd0de06d9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["VAL Epoch End:  {'val_loss': tensor(2.4914, device='cuda:0'), 'val_acc': tensor(0.1516, device='cuda:0'), 'val_rouge1': tensor(0.5092), 'val_rouge2': tensor(0.2953), 'val_rougeL': tensor(0.4152)}\n","TRAIN Epoch End:  {'train_loss': tensor(0.1584, device='cuda:0'), 'train_acc': tensor(0.1474, device='cuda:0'), 'train_rouge1': tensor(0.4553), 'train_rouge2': tensor(0.2435), 'train_rougeL': tensor(0.3628)}\n"]}]},{"cell_type":"code","source":["# 11:10"],"metadata":{"id":"meZqUmxRROSI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bart_args_dict['output_dir']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Rl3N54rL-m5T","outputId":"24185aae-8f44-4253-b02c-7012619208e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'./gdrive/MyDrive/W266 Final Project/models/BART_base-K/'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":300}]},{"cell_type":"code","source":["# Save model\n","# CHANGE THE PATH IF NEEDED\n","bart_model.model.save_pretrained(bart_args_dict['output_dir'] + 'BART_base-1')\n","print(\"Saved model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88FkngvY-krQ","outputId":"63d0dc39-6741-4719-b173-dd00bff37167","executionInfo":{"status":"ok","timestamp":1659229846232,"user_tz":420,"elapsed":2574,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved model\n"]}]},{"cell_type":"code","source":["# 9:02"],"metadata":{"id":"UO4P9kS3Rni_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir='./gdrive/MyDrive/W266 Final Project/models/BART_base-1/'"],"metadata":{"id":"L39I8BbtRnoG","colab":{"base_uri":"https://localhost:8080/","height":822},"outputId":"b91d6a94-a99e-4ee2-b08e-f77f2e588fce"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "]},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"VG5LXQhnCCHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TvA3tMtACCKx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bart Base Eval"],"metadata":{"id":"oGZeYeXyCCel"}},{"cell_type":"code","source":[""],"metadata":{"id":"NyZLxFgEc_JG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scorer = ROUGEScore()\n","\n","accuracy = torchmetrics.Accuracy(mdmc_average = 'samplewise')\n","\n","\n","def get_test_metrics(model, tokenizer, dataloader, file_path):\n","  r_log = open(file_path + \"rouge.txt\", 'w+') #open a text file for logging\n","  b_log = open(file_path + \"bleu.txt\", 'w+') #open a text file for logging\n","  a_log = open(file_path + \"accuracy.txt\", 'w+')\n","  rouge1_li = []\n","  rouge2_li = []\n","  rougeL_li = []\n","  bleu_li = []\n","  accuracy_li = []\n","  # iterate through each batch  in dataloader\n","  # for batch_ind in range(len(dataloader)):\n","  iter_dataloader = iter(dataloader)\n","  # dataloader is preprocessed\n","  max_tokens = 100\n","  i = 0\n","  for batch in iter_dataloader:\n","    \n","\n","    pred_ids = model.generate(input_ids=batch['source_ids'], \n","                                attention_mask=batch['source_mask'],  \n","                                num_beams=4, #9\n","                                    no_repeat_ngram_size=2,\n","                                    num_return_sequences=1,\n","                          #  ,\n","                                    min_length = max_tokens,\n","                                    max_length=max_tokens\n","                                )\n","\n","    #just get rouge metrics\n","    # dec = [tokenizer.decode(ids) for ids in outs]\n","\n","    # get loss and accuracy \n","    # list of strings \n","    pred_m_ids = pred_ids[:, :max_tokens]\n","    pred_str = tokenizer.batch_decode(pred_m_ids, skip_special_tokens=True, \n","                              clean_up_tokenization_spaces=False)\n","    \n","    clean_pred_str = [p.translate(str.maketrans('', '', string.punctuation)).replace(\"\\n\", \" \") for p in pred_str ]\n","\n","    # each sentence needs to be in it's own 2 brackets\n","    clean_pred_str_li = [p.split() for p in clean_pred_str ]\n","\n","    \n","\n","\n","    # texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n","    # # verses with chorus masked\n","    # input_str = tokenizer.batch_decode( batch['source_ids'], skip_special_tokens=True, \n","    #                           clean_up_tokenization_spaces=False)\n","    # targets = [tokenizer.decode(ids) for ids in batch['target_ids']]\n","    target_m_ids =  batch['target_ids'][:, :max_tokens]\n","    target_str = tokenizer.batch_decode(target_m_ids, skip_special_tokens=True, \n","                              clean_up_tokenization_spaces=False)\n","    \n","    clean_target_str = [p.translate(str.maketrans('', '', string.punctuation)).replace(\"\\n\", \" \") for p in target_str ]\n","    \n","    clean_target_str_li = [p.split() for p in clean_target_str ]\n","\n","    # accuracy\n","    # preds_new =  torch.max(pred_m_ids, torch.tensor([1]).cuda())\n","    # labels_new = torch.max(target_m_ids, torch.tensor([1]).cuda())\n","    # 1 accuracy across batch\n","    acc = accuracy(pred_m_ids, target_m_ids).item()\n","    accuracy_li.append(acc)\n","\n","\n","    a_s = \"Batch \" + str(i) + \" = Acc: \" +  str(acc)\n","\n","    a_log.write(a_s + \"\\n\")\n","\n","\n","    # 1 ROUGE SCORE CLACUALTE ACROSS ENTIRE BATCH\n","    rouge_scores = scorer(clean_pred_str, clean_target_str)\n","    rouge1 = rouge_scores['rouge1_fmeasure'].item()\n","    rouge2 = rouge_scores['rouge2_fmeasure'].item()\n","    rougeL = rouge_scores['rougeL_fmeasure'].item()\n","    # print(\"rougue_scores : \", rouge_scores)\n","    # print(\"rouge1 : \", rouge1)\n","    rouge1_li.append(rouge1)\n","    rouge2_li.append(rouge2)\n","    rougeL_li.append(rougeL)\n","    r_s = \"Batch \" + str(i) + \" = Rouge 1: \" + str(rouge1) + \" , Rouge 2: \"  + str(rouge2) + \" , Rouge L: \" + str(rougeL)\n","\n","    # print(r_s)\n","    r_log.write(r_s + \"\\n\")\n","\n","    # print(\"clean_pred_str_li : \")\n","    print(\"PRED : \",   clean_pred_str)\n","    # print(\"LEN pred: \", len(clean_pred_str_li))\n","    # print(\"LEN pred 0: \", len(clean_pred_str_li[0]))\n","   \n","    # print(\"clean_target_str_li : \")\n","    print(\"TARGET : \", clean_target_str)\n","    # print(\"LEN target : \", len(clean_target_str_li))\n","    # print(\"LEN target 0: \", len(clean_target_str_li[0]))\n","    # 1 score for the entire batch\n","\n","    batch_bleu = []\n","    for b_ind in range(len(clean_target_str_li)):\n","      b = sentence_bleu([clean_target_str_li[b_ind]], clean_pred_str_li[b_ind])\n","      batch_bleu.append(b)\n","\n","    bleu = np.average(batch_bleu)\n","    bleu_li.append(bleu)\n","    b_s = \"Batch \" + str(i) + \" = Bleu: \" + str(bleu)\n","    # print(b_s)\n","    b_log.write(b_s + \"\\n\") #write to log\n","\n","    print(\"Batch \" + str(i) + \" = Rouge 1: \" + str(rouge1) + \" , Rouge 2: \"  + str(rouge2) + \" , Rouge L: \" + str(rougeL) \\\n","              + \" , Bleu: \" + str(bleu) +  \" , Acc: \" + str(acc))\n","  \n","    i +=1\n","\n","  avg_rouge1 = np.average(rouge1_li)\n","  avg_rouge2 = np.average(rouge2_li)\n","  avg_rougeL = np.average(rougeL_li)\n","  avg_bleu = np.average(bleu_li)\n","  avg_acc = np.average(accuracy_li)\n","\n","  b_log.close()\n","  r_log.close()\n","  a_log.close()\n","\n","  return avg_rouge1, avg_rouge2, avg_rougeL, avg_bleu, avg_acc\n","\n","\n"],"metadata":{"id":"s01dm5WkC8eF","executionInfo":{"status":"ok","timestamp":1659241473084,"user_tz":420,"elapsed":560,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","gc.collect()\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"ia4qpe7bDvii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","data_args_dict = dict(\n","    data_dir=\"./gdrive/MyDrive/W266 Final Project 2/data/new_data/\",  #directory for all the input data files # path for data files\n","    max_seq_length=256,\n","   \n",")\n","data_args = argparse.Namespace(**data_args_dict)\n"],"metadata":{"id":"yWj6MvrSC8jw","executionInfo":{"status":"ok","timestamp":1659241520125,"user_tz":420,"elapsed":285,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"a5DP5bXCFU-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try BART model\n","bart_base_dir = \"./gdrive/MyDrive/W266 Final Project 2/models/BART_base-1/\"\n","bart_base_model = BartForConditionalGeneration.from_pretrained(bart_base_dir + \"BART_base-1\")\n","bart_base_tokenizer =  BartTokenizer.from_pretrained('facebook/bart-base', model_max_length = 256)\n","\n","\n","\n","bart_base_test_dataset = get_dataset(tokenizer=bart_base_tokenizer, type_path=\"test\", args=data_args, col = \"_b\")\n","bart_base_dataloader = DataLoader(bart_base_test_dataset, batch_size=8, drop_last=False, shuffle=False, num_workers=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jH1_CrjwChCi","outputId":"e4743e31-48cc-4268-946b-d0b249511ac4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"IPnsM0FcDaKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4fChHxzNJhMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bart_base_rouge1, bart_base_rouge2, bart_base_rougeL, bart_base_bleu, bart_base_acc = get_test_metrics(bart_base_model, bart_base_tokenizer, bart_base_dataloader, bart_base_dir + \"test_logs/\")\n"],"metadata":{"id":"F99PVznWCZa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vZroDiOKCZdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"**** BART Full Test Metrics : ****\")\n","print(\"Rouge 1 : \", bart_base_rouge1)\n","print(\"Rouge 2 : \", bart_base_rouge2)\n","print(\"Rouge L : \", bart_base_rougeL)\n","print(\"Bleu : \", bart_base_bleu)\n","print(\"Acc : \", bart_base_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzSIApiqCZhd","outputId":"deae7331-c5b4-48b8-e9a1-83defe4f6629"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["**** BART Full Test Metrics : ****\n","Rouge 1 :  0.16246772\n","Rouge 2 :  0.02335625\n","Rouge L :  0.10855109\n","Bleu :  0.00901324356546141\n","Acc :  0.018910605\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"4nuvSksfCZkX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PwEkFpqkAde5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"60erICeadxP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7AJKWi27dxcD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# T5 "],"metadata":{"id":"aXDPBiaydyD9"}},{"cell_type":"code","source":["def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"metadata":{"id":"UfSSSDETeArw","executionInfo":{"status":"ok","timestamp":1659229949038,"user_tz":420,"elapsed":410,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class T5FineTuner(pl.LightningModule):\n","  def __init__(self, hparams):\n","    super(T5FineTuner, self).__init__()\n","    # self.hparams = hparams\n","    self.hparams.update(vars(hparams))\n","    self.save_hyperparameters()\n","    \n","    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, model_max_length = hparams.max_seq_length )\n","    self.acc = torchmetrics.Accuracy(mdmc_average = 'samplewise')\n","    self.scorer = ROUGEScore() #rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n","  \n","  def is_logger(self):\n","    return self.trainer.global_rank <= 0\n","  \n","  def forward(\n","      # doesn't have a max_length method, so it will keep trying to predict the same length as the target ids\n","      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n","  ):\n","    return self.model(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        decoder_input_ids=decoder_input_ids,\n","        decoder_attention_mask=decoder_attention_mask,\n","        labels=labels,\n","    )\n","\n","  def _step(self, batch):\n","    labels = batch[\"target_ids\"]\n","    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","    outputs = self(\n","        input_ids=batch[\"source_ids\"],\n","        attention_mask=batch[\"source_mask\"],\n","        # takes in labels and shifts right to create decoder input ids shift\n","        labels=labels,\n","        decoder_attention_mask=batch['target_mask']\n","    )\n","\n","    loss = outputs[0]\n","    # print(\"Output keys: \", outputs.keys())\n","    # print(\"Batch: \" , batch)\n","    # print(\"Batch shape :\" , batch.shape)\n","\n","\n","    # source = batch['source_ids']\n","\n","    #  Pred Output size :  torch.Size([8, 256, 32128])\n","    # Target Labels size :  torch.Size([8, 256])\n","\n","\n","    # values are the probabilities.\n","    # indices are the indices\n","    # y_pred = y_max_idx  # predictions are really the inx \\in [n_classes] with the highest scores\n","    \n","    \n","\n","\n","  \n","    \n","    # print(\"******\")\n","    # print(\"Pred Output size : \", preds.size())\n","    # print(\"Target Labels size : \", torch.max(labels, torch.tensor([0])).size())\n","\n","\n","    pred_ids = torch.max(outputs['logits'], 2).indices\n","\n","    # change pred ids index of blank string token at end to index of PAD token, like target label has\n","    # go backward\n","    for i in range(pred_ids.size(0)):\n","      for j in range(1, pred_ids.size(1) + 1):\n","        # if  value is  3, then replace current valuw ith 0 and break\n","        # print(k[i][j])\n","        if pred_ids[i][-j] == 3:\n","          pred_ids[i][-j]=0\n","        else:\n","          break\n","  \n","\n","    # change all the empty strings (word ind = 3) at the end to 0's to represent PAD token ind.\n","    # for each tensor in pred_ids, maks only \n","\n","    pred_str_li = self.tokenizer.batch_decode(pred_ids,skip_special_tokens=True, clean_up_tokenization_spaces=False)\n","    \n","    # get the values before the cast. set -100 to 0 . Pad token or 0 is the same, it will be ignored in string\n","    target_ids = torch.max(batch[\"target_ids\"], torch.tensor([0]).cuda())\n","    # print(\"target_ids : \", target_ids.size())\n","    # print(\"pred_ids : \", pred_ids.size())\n","    # print(\"Tokenizer max length \", self.tokenizer.model_max_length)\n","\n","    # print(\"target_ids arr : \", target_ids.shape)\n","    # print(\"pred_ids arr : \", pred_ids.shape)\n","\n","\n","\n","    target_str_li = self.tokenizer.batch_decode(target_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n"," \n","    \n","    # predicts what should be in the mask\n","    \n","    # get rogue scores for each string pair and avergae it\n","    \n","    # rogue score for bathc is average of that batch, loss laredy calculatd that average\n","\n","    # rouge(preds, target)\n","\n","    rouge_scores = self.scorer(pred_str_li, target_str_li)\n","    # torch.mean(torch.Tensor([self.scorer.score(target_str_li[i], pred_str_li[i])['rouge1'] for i in range(len(pred_str_li))]))\n","    #get f1 score of everything\n","\n","    rouge1 = rouge_scores['rouge1_fmeasure']\n","    rouge2 = rouge_scores['rouge2_fmeasure']\n","    rougeL = rouge_scores['rougeL_fmeasure']\n","    # rougeLsum = rouge_scores['rougeLsum_fmeasure']\n","\n","  \n","\n","    preds_new =  torch.max(pred_ids, torch.tensor([0]).cuda())\n","    labels_new = torch.max(labels, torch.tensor([0]).cuda())\n","\n","    # set -100 to 0's\n","    acc = self.acc(preds_new, labels_new)\n","    # self.log('training/step_acc', acc)\n","    # print(\"Target Output : \", target_str_li)\n","    # print(\"Pred Output : \", pred_str_li)\n","    \n","    # print(\"LOSS\", loss)\n","    # print(\"Accuracy\", acc)\n","\n","\n","\n","\n","\n","    return loss, acc, rouge1, rouge2, rougeL\n","\n","  def training_step(self, batch, batch_idx):\n","    loss, acc, rouge1, rouge2, rougeL = self._step(batch)\n","\n","    tensorboard_logs = {\"train_loss\": loss, \"train_acc\": acc, \"train_rouge1\": rouge1, \"train_rouge2\": rouge2, \"train_rougeL\": rougeL  }\n","\n","    self.log('log', tensorboard_logs,  prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","\n","    # print(\"TRAIN STEP: \", tensorboard_logs)\n","\n","    \n","    return {\"loss\": loss, \"acc\": acc,   \"rouge1\": rouge1, \"rouge2\": rouge2,  \"rougeL\": rougeL, \"log\": tensorboard_logs}\n","  \n","  def training_epoch_end(self, outputs):\n","    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","    avg_train_acc = torch.stack([x[\"acc\"] for x in outputs]).mean()\n","    avg_train_rouge1 = torch.stack([x[\"rouge1\"] for x in outputs]).mean()\n","    avg_train_rouge2 = torch.stack([x[\"rouge2\"] for x in outputs]).mean()\n","    avg_train_rougeL = torch.stack([x[\"rougeL\"] for x in outputs]).mean()\n","\n","    tensorboard_logs = {\"train_loss\": avg_train_loss, \"train_acc\": avg_train_acc, \"train_rouge1\": avg_train_rouge1, \"train_rouge2\": avg_train_rouge2, \"train_rougeL\": avg_train_rougeL }\n","\n","    self.log('step', self.trainer.current_epoch)\n","    self.log('avg_train_loss', avg_train_loss)\n","    self.log('avg_train_acc', avg_train_acc)\n","    \n","    self.log('avg_train_rouge1', avg_train_rouge1)\n","    self.log('avg_train_rouge2', avg_train_rouge2)\n","    self.log('avg_train_rougeL', avg_train_rougeL)\n","    self.log('log', tensorboard_logs)\n","    self.log('progress_bar', tensorboard_logs)\n","\n"," \n","    print(\"TRAIN Epoch End: \", tensorboard_logs)\n","\n","    # return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","  def validation_step(self, batch, batch_idx):\n","    loss, acc, rouge1, rouge2, rougeL = self._step(batch)\n","    tensorboard_logs = {\"val_loss\": loss, \"val_acc\": acc, \"val_rouge1\": rouge1, \"val_rouge2\": rouge2, \"val_rougeL\": rougeL}\n","\n","    # print(\"VAL STEP: \", {tensorboard_logs)\n","    self.log('log', tensorboard_logs,  prog_bar=True, logger=True, on_step=True, on_epoch=True)\n","\n","    return {\"val_loss\": loss, \"val_acc\": acc, \"val_rouge1\": rouge1, \"val_rouge2\": rouge2, \"val_rougeL\": rougeL}\n","\n","    \n","  def validation_epoch_end(self, outputs):\n","    \n","    avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","    avg_val_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n","    avg_val_rouge1 = torch.stack([x[\"val_rouge1\"] for x in outputs]).mean()\n","    avg_val_rouge2 = torch.stack([x[\"val_rouge2\"] for x in outputs]).mean()\n","    avg_val_rougeL = torch.stack([x[\"val_rougeL\"] for x in outputs]).mean()\n","\n","    # print(\"VAL EPOCH END  Keys: \", outputs.keys() )\n","    \n","    tensorboard_logs = {\"val_loss\": avg_val_loss, \"val_acc\": avg_val_acc, \"val_rouge1\": avg_val_rouge1, \"val_rouge2\": avg_val_rouge2, \"val_rougeL\": avg_val_rougeL}\n","    # return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    self.log('avg_val_loss', avg_val_loss)\n","    self.log('avg_val_acc', avg_val_acc)\n","    self.log('avg_val_rouge1', avg_val_rouge1)\n","    self.log('avg_val_rouge2', avg_val_rouge2)\n","    self.log('avg_val_rougeL', avg_val_rougeL)\n","\n","    self.log('log', tensorboard_logs)\n","    self.log('progress_bar', tensorboard_logs)\n","\n","    print(\"VAL Epoch End: \", tensorboard_logs)\n","\n","\n","  def configure_optimizers(self):\n","    \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","    model = self.model\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": self.hparams.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","    self.opt = optimizer\n","    return [optimizer]\n","  \n","  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure=None, \n","                    on_tpu=False, using_native_amp=False, \n","                    using_lbfgs=False):\n","    # if on_tpu == True: #self.trainer.use_tpu:\n","    #   # xm.optimizer_step(optimizer)\n","    #   optimizer.step()\n","    # else:\n","    # optimizer.step(closure=optimizer_closure)\n","    optimizer.step(closure=optimizer_closure)\n","    optimizer.zero_grad()\n","    self.lr_scheduler.step()\n","  \n","  def get_tqdm_dict(self):\n","    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","    return tqdm_dict\n","\n","  def train_dataloader(self):\n","    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams, col = \"\")\n","    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=False, shuffle=False, num_workers=4)\n","    t_total = (\n","        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","        // self.hparams.gradient_accumulation_steps\n","        * float(self.hparams.num_train_epochs)\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","    )\n","    self.lr_scheduler = scheduler\n","    return dataloader\n","\n","  def val_dataloader(self):\n","    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams, col = \"\")\n","    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"],"metadata":{"id":"5BApCl1HeAzS","executionInfo":{"status":"ok","timestamp":1659229969878,"user_tz":420,"elapsed":1029,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["logger = logging.getLogger(__name__)\n","\n","\n","\n","class LoggingCallback(pl.Callback):\n","  def on_validation_end(self, trainer, pl_module):\n","    logger.info(\"***** Validation results *****\")\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","      # Log results\n","      output_val_results_file = os.path.join(pl_module.hparams.output_dir, \"val_results.txt\")\n","      with open(output_val_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          # if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","          writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","  def on_test_end(self, trainer, pl_module):\n","    logger.info(\"***** Test results *****\")\n","\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","      with open(output_test_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          #if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","          writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"],"metadata":{"id":"7T2lmCP-eIAF","executionInfo":{"status":"ok","timestamp":1659229983860,"user_tz":420,"elapsed":329,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["t5_args_dict = dict(\n","    data_dir=\"./gdrive/MyDrive/W266 Final Project 2/data/new_data\",  #directory for all the input data files # path for data files\n","    output_dir=\"./gdrive/MyDrive/W266 Final Project 2/models/T5_base-1/\", # path to save the checkpoints\n","    model_name_or_path='t5-base',\n","    tokenizer_name_or_path='t5-base',\n","    max_seq_length=256,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=8,\n","    eval_batch_size=8,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=16,\n","    n_gpu= 1,#1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    amp_backend = \"apex\",\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")"],"metadata":{"id":"mSc7xfoOehNK","executionInfo":{"status":"ok","timestamp":1659229998268,"user_tz":420,"elapsed":256,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# args_dict.update({'data_dir': 'T5', 'output_dir': 'outputs', 'num_train_epochs':2})\n","t5_args = argparse.Namespace(**t5_args_dict)\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=t5_args.output_dir, filename='{epoch}-{val_loss:.2f}', monitor=\"avg_val_loss\", mode=\"min\", save_top_k=5\n",")\n","\n","# path to save lightning_logs\n","tb_logger = pl_loggers.TensorBoardLogger(save_dir=t5_args.output_dir)\n","\n","\n","t5_train_params = dict(\n","    logger = tb_logger,\n","    \n","    accumulate_grad_batches=t5_args.gradient_accumulation_steps,\n","\n","\n","    gpus=t5_args.n_gpu, \n","    # accelerator = \"cpu\",\n","\n","    max_epochs=t5_args.num_train_epochs,\n","    log_every_n_steps = 50, # change based on size of dataset\n","    # early_stop_callback=False,\n","    precision= 16 if t5_args.fp_16 else 32,\n","    amp_level=t5_args.opt_level,\n","    amp_backend = t5_args.amp_backend,\n","    gradient_clip_val=t5_args.max_grad_norm,\n","    enable_checkpointing = True,\n","    # checkpoint_callback=checkpoint_callback,\n","    callbacks=[checkpoint_callback, LoggingCallback()],\n",")\n","\n"],"metadata":{"id":"QbdH4V7aehSW","executionInfo":{"status":"ok","timestamp":1659230014601,"user_tz":420,"elapsed":4352,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"v9hI-JsSe-DW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune T5"],"metadata":{"id":"vTMvKojge-ZF"}},{"cell_type":"code","source":["t5_model = T5FineTuner(t5_args)"],"metadata":{"id":"hp_P-zriehUt","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["9e508301b35a44f6ae2feb97c5cd8c27","f21d5214d1db423997beed05f258ed82","eb957d335d6a41d1986ae27f20b80340","843f56010ea6460a93ab28971345f00a","0f3118d5574848458d8e554459b41357","6ae25b98fb9c4435a921237c0decccc7","75fb360e338d494bb8550d4f38e938b4","546115ae0cd543bc83aef1011eb5d220","ea701a535cfd498696adb90a84e71f84","f1cbb3580e664314ad5be4419a6ecd5e","3d50a661f8624e1083e573a7fe08b0af","44473f635bc3419381608f72cda418fe","fb0ea0e595654a3ead9a311794dea47e","6fe996b860b34909b51bb9210280872b","904aa7e8b0b643ed9f7391f2d1fd30a4","56cea87687ab46dda1b3b9126de1e502","49cd8cf68c484c5889664210490ae7ad","1b8216b98f2d482b92446880a3f3dce0","46b665a9ad8e4c8d80f072968a107a53","c90fe3d0ed5f44b48a5e67d0441c955a","c934af66a07e498bba191fa203392da0","09b036ce880a4fd1a3d6259d646a63ad","cba037101ff1465396efb9c48f7d58a3","66a956396f3e408abd89de8310be20d0","1cd1e4850ecf4e359b5074616ec42eea","ad05445d641e4035894ff3400e1d7db0","15763b0d027e4d8fbee270cdbf50b040","e32f2380b5244e548bf4110872a5f533","38356a838dfe4fd78691b414985af781","d25e23e400c14527b25274b56dcda70b","4c434b3d44484f1aabbb7b36aaeec64b","47194f8c1d1f4a36af4dc920cb43a2c1","d4e71e9bff6340e4b981a4ed202bd1da"]},"executionInfo":{"status":"ok","timestamp":1659230055240,"user_tz":420,"elapsed":29368,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}},"outputId":"2165e707-7be8-4cd1-df61-fff886c942ba"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e508301b35a44f6ae2feb97c5cd8c27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/850M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44473f635bc3419381608f72cda418fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba037101ff1465396efb9c48f7d58a3"}},"metadata":{}}]},{"cell_type":"code","source":["t5_trainer = pl.Trainer(**t5_train_params)"],"metadata":{"id":"VmeacPV4ehXS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659230062278,"user_tz":420,"elapsed":374,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}},"outputId":"da2db59d-f8c3-4c31-a7d0-3e2244522be3"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]}]},{"cell_type":"code","source":["# # # clear torch cache\n","\n","gc.collect()\n","\n","torch.cuda.empty_cache()\n","\n","# # #2:14"],"metadata":{"id":"OPuTAtNqeIH_","executionInfo":{"status":"ok","timestamp":1659240251049,"user_tz":420,"elapsed":933,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# start fine-tuning\n","t5_trainer.fit(t5_model)"],"metadata":{"id":"VZLKooZtAdh3","colab":{"base_uri":"https://localhost:8080/","height":588,"referenced_widgets":["bc36d28bfc794d7dbeb9a6e8a317bacc","82d2f9b2003648e1afc82e71c63fdbc5","c3e829476ef74217994c535db08c1b0d","be0ec10779394df78ffde554a609518c","6cbad8ac4d0442f896749d9fec825f06","79201ff1decd434caaf6f7e4f9b1b9bf","6c024ae1a6974760836ec9e4fe074463","0bc06e44c6e54cc2a6b3aae1634687f5","633ef215bd7445169dede22d9656b7b7","0418a7df5f5d4fba8a69c44a15593b6e","13e04c06b4ab4bd88a44b19987f57536","04db8b4ccbc44d56a22b2ef1671acff9","c3a44a336a764a13a9cea52e6fb96b26","d9a05faffad742efbf1e3bdecbdca6ad","cf913c363b3e4f43b7181aa177f457a3","c8580717b95c4f39a521b2f83d4908c8","b8be6459810b43cab66622902702a899","53a8fb902d8e407ba9e395a9dc3c11ea","ee2fea5e467b4b528a84c69f6e5cb14a","9a60c97445f9402099004d9dbdceef39","f72434aa3c7047e3b9a02736940a459e","14e54f56cbdc4eca8d7b3461f036f6fc","48dd71a1d110434f833c0d820f0acc87","12cbaa1f3b8047d599490de80c5a245f","a12a5a47cec9492eb9ca7a999a9630f1","b33cebf0e921421eabbfc3b2a9eed147","fd0626a9068b41baa5ffd7fca103c3d4","df6b3d0aa6c94603b726e4ae2391d7a2","e22ff3611df24f449c973fcecbbadf6b","41e88a638e4245e4924e29c26530c8db","888302c78468498ea4ea7e1e7e020047","35f230da920d4644a5392b6ef034e652","63692891b6274e42a48f917a3fde3eef"]},"executionInfo":{"status":"ok","timestamp":1659239810545,"user_tz":420,"elapsed":9736996,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}},"outputId":"4f0988ef-6971-40ed-a5c0-d65e1df55e8f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/configuration_validator.py:118: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n","  \"When using `Trainer(accumulate_grad_batches != 1)` and overriding\"\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /content/gdrive/.shortcut-targets-by-id/1tdlBf0ibenbP95uaYV-bH3xJJIgmACVX/W266 Final Project 2/models/T5_base-1 exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","\n","  | Name   | Type                       | Params\n","------------------------------------------------------\n","0 | model  | T5ForConditionalGeneration | 222 M \n","1 | acc    | Accuracy                   | 0     \n","2 | scorer | ROUGEScore                 | 0     \n","------------------------------------------------------\n","222 M     Trainable params\n","0         Non-trainable params\n","222 M     Total params\n","891.614   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc36d28bfc794d7dbeb9a6e8a317bacc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["VAL Epoch End:  {'val_loss': tensor(6.0126, device='cuda:0'), 'val_acc': tensor(0.4800, device='cuda:0'), 'val_rouge1': tensor(0.2774), 'val_rouge2': tensor(0.1154), 'val_rougeL': tensor(0.2435)}\n"]},{"output_type":"display_data","data":{"text/plain":["Training: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04db8b4ccbc44d56a22b2ef1671acff9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Validation: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48dd71a1d110434f833c0d820f0acc87"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:287: UserWarning: The ``compute`` method of metric _ResultMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n","  f\"The ``compute`` method of metric {self.__class__.__name__}\"\n"]},{"output_type":"stream","name":"stdout","text":["VAL Epoch End:  {'val_loss': tensor(2.2634, device='cuda:0'), 'val_acc': tensor(0.5496, device='cuda:0'), 'val_rouge1': tensor(0.5504), 'val_rouge2': tensor(0.3241), 'val_rougeL': tensor(0.4787)}\n","TRAIN Epoch End:  {'train_loss': tensor(0.1639, device='cuda:0'), 'train_acc': tensor(0.4440, device='cuda:0'), 'train_rouge1': tensor(0.5031), 'train_rouge2': tensor(0.2585), 'train_rougeL': tensor(0.4292)}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n","  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"]}]},{"cell_type":"code","source":["t5_model.model.save_pretrained(t5_args_dict['output_dir'] + 'T5_base-1')\n","print(\"Saved model\")"],"metadata":{"id":"Q8Gdc_mUf7Z3","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"error","timestamp":1659241486010,"user_tz":420,"elapsed":1333,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}},"outputId":"803bf729-7803-4b44-b780-f0b69d4b1023"},"execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-226f08fa193e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt5_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5_args_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'T5_base-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 't5_model' is not defined"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Hgi6hnQqf7cM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## T5 Eval"],"metadata":{"id":"z8WGTkZoAeCo"}},{"cell_type":"code","source":["# Try T5 model\n","t5_base_dir = \"./gdrive/MyDrive/W266 Final Project 2/models/T5_base-1/\"\n","t5_base_model = T5ForConditionalGeneration.from_pretrained(t5_base_dir + \"/T5_base-1\")\n","t5_base_tokenizer =  T5Tokenizer.from_pretrained('t5-base', model_max_length = 256)\n","\n","\n","t5_base_test_dataset = get_dataset(tokenizer=t5_base_tokenizer, type_path=\"test\", args=data_args, col = \"\")\n","t5_base_dataloader = DataLoader(t5_base_test_dataset, batch_size=16, drop_last=False, shuffle=False, num_workers=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O-SUcIS5AggR","outputId":"36326157-50cb-457c-fc91-be9fa55e8a24","executionInfo":{"status":"ok","timestamp":1659241541874,"user_tz":420,"elapsed":15034,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["\n","t5_base_rouge1, t5_base_rouge2, t5_base_rougeL, t5_base_bleu, t5_base_acc = get_test_metrics(t5_base_model, t5_base_tokenizer, t5_base_dataloader, t5_base_dir + \"test_logs/\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"dDjj1fIkAgrK","outputId":"e6c02389-53ac-483b-d60e-37a9f1b5677c","executionInfo":{"status":"error","timestamp":1659241614961,"user_tz":420,"elapsed":66625,"user":{"displayName":"Amangeet Samra","userId":"04102851300131370151"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-5cc88505f797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt5_base_rouge1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_rouge2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_rougeL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5_base_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt5_base_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"test_logs/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-ef7328174cfe>\u001b[0m in \u001b[0;36mget_test_metrics\u001b[0;34m(model, tokenizer, dataloader, file_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m                           \u001b[0;31m#  ,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                     \u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                                 )\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2216\u001b[0m             )\n\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1651\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m         )\n\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 )\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         )\n\u001b[1;32m    581\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    503\u001b[0m         )\n\u001b[1;32m    504\u001b[0m         value_states = project(\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_value_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         )\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mproject\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0;31m# self-attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m                     \u001b[0;31m# (batch_size, n_heads, key_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0;31m# cross-attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(\"**** T5 Full Test Metrics : ****\")\n","print(\"Rouge 1 : \", t5_base_rouge1)\n","print(\"Rouge 2 : \", t5_base_rouge2)\n","print(\"Rouge L : \", t5_base_rougeL)\n","print(\"Bleu : \", t5_base_bleu)\n","print(\"Acc : \", t5_base_acc)"],"metadata":{"id":"VH_QiKvUgtLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NOhhwHTagtSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"39iMuyNDg8_t"},"execution_count":null,"outputs":[]}]}